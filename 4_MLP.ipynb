{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/HR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>part</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years   part  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   part                   14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sales', 'accounting', 'hr', 'technical', 'support', 'management',\n",
       "       'IT', 'product_mng', 'marketing', 'RandD'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.part.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['low', 'medium', 'high'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.salary.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary  part       \n",
       "high    IT               83\n",
       "        RandD            51\n",
       "        accounting       74\n",
       "        hr               45\n",
       "        management      225\n",
       "        marketing        80\n",
       "        product_mng      68\n",
       "        sales           269\n",
       "        support         141\n",
       "        technical       201\n",
       "low     IT              609\n",
       "        RandD           364\n",
       "        accounting      358\n",
       "        hr              335\n",
       "        management      180\n",
       "        marketing       402\n",
       "        product_mng     451\n",
       "        sales          2099\n",
       "        support        1146\n",
       "        technical      1372\n",
       "medium  IT              535\n",
       "        RandD           372\n",
       "        accounting      335\n",
       "        hr              359\n",
       "        management      225\n",
       "        marketing       376\n",
       "        product_mng     383\n",
       "        sales          1772\n",
       "        support         942\n",
       "        technical      1147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['salary','part']).size()#分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(pd.get_dummies(data.salary))#独热编码one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(pd.get_dummies(data.part))#独热编码one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>...</th>\n",
       "      <th>IT</th>\n",
       "      <th>RandD</th>\n",
       "      <th>accounting</th>\n",
       "      <th>hr</th>\n",
       "      <th>management</th>\n",
       "      <th>marketing</th>\n",
       "      <th>product_mng</th>\n",
       "      <th>sales</th>\n",
       "      <th>support</th>\n",
       "      <th>technical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14999 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "0                    0.38             0.53               2   \n",
       "1                    0.80             0.86               5   \n",
       "2                    0.11             0.88               7   \n",
       "3                    0.72             0.87               5   \n",
       "4                    0.37             0.52               2   \n",
       "...                   ...              ...             ...   \n",
       "14994                0.40             0.57               2   \n",
       "14995                0.37             0.48               2   \n",
       "14996                0.37             0.53               2   \n",
       "14997                0.11             0.96               6   \n",
       "14998                0.37             0.52               2   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident  left  \\\n",
       "0                       157                   3              0     1   \n",
       "1                       262                   6              0     1   \n",
       "2                       272                   4              0     1   \n",
       "3                       223                   5              0     1   \n",
       "4                       159                   3              0     1   \n",
       "...                     ...                 ...            ...   ...   \n",
       "14994                   151                   3              0     1   \n",
       "14995                   160                   3              0     1   \n",
       "14996                   143                   3              0     1   \n",
       "14997                   280                   4              0     1   \n",
       "14998                   158                   3              0     1   \n",
       "\n",
       "       promotion_last_5years  high  low  ...  IT  RandD  accounting  hr  \\\n",
       "0                          0     0    1  ...   0      0           0   0   \n",
       "1                          0     0    0  ...   0      0           0   0   \n",
       "2                          0     0    0  ...   0      0           0   0   \n",
       "3                          0     0    1  ...   0      0           0   0   \n",
       "4                          0     0    1  ...   0      0           0   0   \n",
       "...                      ...   ...  ...  ...  ..    ...         ...  ..   \n",
       "14994                      0     0    1  ...   0      0           0   0   \n",
       "14995                      0     0    1  ...   0      0           0   0   \n",
       "14996                      0     0    1  ...   0      0           0   0   \n",
       "14997                      0     0    1  ...   0      0           0   0   \n",
       "14998                      0     0    1  ...   0      0           0   0   \n",
       "\n",
       "       management  marketing  product_mng  sales  support  technical  \n",
       "0               0          0            0      1        0          0  \n",
       "1               0          0            0      1        0          0  \n",
       "2               0          0            0      1        0          0  \n",
       "3               0          0            0      1        0          0  \n",
       "4               0          0            0      1        0          0  \n",
       "...           ...        ...          ...    ...      ...        ...  \n",
       "14994           0          0            0      0        1          0  \n",
       "14995           0          0            0      0        1          0  \n",
       "14996           0          0            0      0        1          0  \n",
       "14997           0          0            0      0        1          0  \n",
       "14998           0          0            0      0        1          0  \n",
       "\n",
       "[14999 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11428\n",
       "1     3571\n",
       "Name: left, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.left.value_counts()#统计left不同值的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = data.left.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.from_numpy(Y_data).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data= data[[c for c in data.columns if c !='left']].values#列表推导式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_data).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14999, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14999, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义模型：  \n",
    "__int__: 初始化所有的层  \n",
    "forward：定义模型的运算过程（前向传播过程）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):                        #不是model\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(20,64)       #输入20输出64\n",
    "        self.linear_2 = nn.Linear(64,64)\n",
    "        self.linear_3 = nn.Linear(64,1)        #逻辑回归最终输出为1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,input):\n",
    "        x = self.linear_1(input)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(input)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_3(input)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改写模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):                        #不是model\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(20,64)       #输入20输出64\n",
    "        self.linear_2 = nn.Linear(64,64)\n",
    "        self.linear_3 = nn.Linear(64,1)        #逻辑回归最终输出为1\n",
    "    def forward(self,input):\n",
    "        x = F.relu(self.linear_1(input))\n",
    "        x = F.relu((self.linear_2(x)))\n",
    "        x = F.sigmoid((self.linear_3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (linear_1): Linear(in_features=20, out_features=64, bias=True)\n",
       "  (linear_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (linear_3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Model()\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=learning)\n",
    "    return model,opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ,optim = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.5927024483680725\n",
      "epoch 1 loss 0.6552059650421143\n",
      "epoch 2 loss 0.5867846012115479\n",
      "epoch 3 loss 0.6032168865203857\n",
      "epoch 4 loss 0.6233773827552795\n",
      "epoch 5 loss 0.6074861884117126\n",
      "epoch 6 loss 0.6510524153709412\n",
      "epoch 7 loss 0.6319971680641174\n",
      "epoch 8 loss 0.5774888396263123\n",
      "epoch 9 loss 0.6484155058860779\n",
      "epoch 10 loss 0.6515631675720215\n",
      "epoch 11 loss 0.6164216995239258\n",
      "epoch 12 loss 0.5874398350715637\n",
      "epoch 13 loss 0.5665642619132996\n",
      "epoch 14 loss 0.5552641153335571\n",
      "epoch 15 loss 0.5507842302322388\n",
      "epoch 16 loss 0.5493012070655823\n",
      "epoch 17 loss 0.5489226579666138\n",
      "epoch 18 loss 0.5489834547042847\n",
      "epoch 19 loss 0.548973560333252\n",
      "epoch 20 loss 0.5490770936012268\n",
      "epoch 21 loss 0.5490998029708862\n",
      "epoch 22 loss 0.5492899417877197\n",
      "epoch 23 loss 0.5492200255393982\n",
      "epoch 24 loss 0.5492886900901794\n",
      "epoch 25 loss 0.54923415184021\n",
      "epoch 26 loss 0.5493602752685547\n",
      "epoch 27 loss 0.549354076385498\n",
      "epoch 28 loss 0.5492085814476013\n",
      "epoch 29 loss 0.5492201447486877\n",
      "epoch 30 loss 0.5492309927940369\n",
      "epoch 31 loss 0.5492196083068848\n",
      "epoch 32 loss 0.5492090582847595\n",
      "epoch 33 loss 0.5494153499603271\n",
      "epoch 34 loss 0.5493456125259399\n",
      "epoch 35 loss 0.5493842959403992\n",
      "epoch 36 loss 0.5493072867393494\n",
      "epoch 37 loss 0.549268901348114\n",
      "epoch 38 loss 0.5491841435432434\n",
      "epoch 39 loss 0.549209475517273\n",
      "epoch 40 loss 0.549347996711731\n",
      "epoch 41 loss 0.5493336915969849\n",
      "epoch 42 loss 0.5492892265319824\n",
      "epoch 43 loss 0.5491931438446045\n",
      "epoch 44 loss 0.5491729378700256\n",
      "epoch 45 loss 0.5493572354316711\n",
      "epoch 46 loss 0.5493267178535461\n",
      "epoch 47 loss 0.5492390990257263\n",
      "epoch 48 loss 0.5491849780082703\n",
      "epoch 49 loss 0.5491631031036377\n",
      "epoch 50 loss 0.5493003726005554\n",
      "epoch 51 loss 0.5493164658546448\n",
      "epoch 52 loss 0.5492284297943115\n",
      "epoch 53 loss 0.5491264462471008\n",
      "epoch 54 loss 0.5491527318954468\n",
      "epoch 55 loss 0.5492900013923645\n",
      "epoch 56 loss 0.5492639541625977\n",
      "epoch 57 loss 0.5491706728935242\n",
      "epoch 58 loss 0.5491353273391724\n",
      "epoch 59 loss 0.5491161942481995\n",
      "epoch 60 loss 0.5490812063217163\n",
      "epoch 61 loss 0.5492280125617981\n",
      "epoch 62 loss 0.5492038726806641\n",
      "epoch 63 loss 0.5492540001869202\n",
      "epoch 64 loss 0.549194872379303\n",
      "epoch 65 loss 0.5491927862167358\n",
      "epoch 66 loss 0.5491927862167358\n",
      "epoch 67 loss 0.5491926074028015\n",
      "epoch 68 loss 0.5491857528686523\n",
      "epoch 69 loss 0.5491651892662048\n",
      "epoch 70 loss 0.5491517782211304\n",
      "epoch 71 loss 0.5491280555725098\n",
      "epoch 72 loss 0.5491257309913635\n",
      "epoch 73 loss 0.5491054654121399\n",
      "epoch 74 loss 0.5490254163742065\n",
      "epoch 75 loss 0.5492260456085205\n",
      "epoch 76 loss 0.549217164516449\n",
      "epoch 77 loss 0.5491852164268494\n",
      "epoch 78 loss 0.5491546988487244\n",
      "epoch 79 loss 0.5490778684616089\n",
      "epoch 80 loss 0.5491156578063965\n",
      "epoch 81 loss 0.549034595489502\n",
      "epoch 82 loss 0.5492310523986816\n",
      "epoch 83 loss 0.54915851354599\n",
      "epoch 84 loss 0.5491957664489746\n",
      "epoch 85 loss 0.5491135120391846\n",
      "epoch 86 loss 0.5490809679031372\n",
      "epoch 87 loss 0.5490699410438538\n",
      "epoch 88 loss 0.5490354895591736\n",
      "epoch 89 loss 0.5491812825202942\n",
      "epoch 90 loss 0.5491482615470886\n",
      "epoch 91 loss 0.5491372346878052\n",
      "epoch 92 loss 0.549092710018158\n",
      "epoch 93 loss 0.5490095019340515\n",
      "epoch 94 loss 0.5490471124649048\n",
      "epoch 95 loss 0.5491921305656433\n",
      "epoch 96 loss 0.5491513013839722\n",
      "epoch 97 loss 0.5491388440132141\n",
      "epoch 98 loss 0.5490553379058838\n",
      "epoch 99 loss 0.5490187406539917\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        start = i*batch\n",
    "        end = start + batch\n",
    "        x = X[start:end]\n",
    "        y = Y[start:end]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch',epoch,'loss',loss_fn(model(X),Y).data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5490, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(X),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用dataset类进行重构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRdataset = TensorDataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x2310509d3c8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HRdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(HRdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1000e-01, 8.8000e-01, 7.0000e+00, 2.7200e+02, 4.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [7.2000e-01, 8.7000e-01, 5.0000e+00, 2.2300e+02, 5.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7000e-01, 5.2000e-01, 2.0000e+00, 1.5900e+02, 3.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HRdataset[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ,optim = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.5840166211128235\n",
      "epoch 1 loss 0.5937082171440125\n",
      "epoch 2 loss 0.5775942802429199\n",
      "epoch 3 loss 0.5740918517112732\n",
      "epoch 4 loss 0.6236390471458435\n",
      "epoch 5 loss 0.6131027936935425\n",
      "epoch 6 loss 0.6411235928535461\n",
      "epoch 7 loss 0.6231314539909363\n",
      "epoch 8 loss 0.5792242288589478\n",
      "epoch 9 loss 0.5579831004142761\n",
      "epoch 10 loss 0.5511090755462646\n",
      "epoch 11 loss 0.5490661859512329\n",
      "epoch 12 loss 0.548747181892395\n",
      "epoch 13 loss 0.548991322517395\n",
      "epoch 14 loss 0.5488781929016113\n",
      "epoch 15 loss 0.54966139793396\n",
      "epoch 16 loss 0.5490298271179199\n",
      "epoch 17 loss 0.5489349961280823\n",
      "epoch 18 loss 0.5490275621414185\n",
      "epoch 19 loss 0.5489771366119385\n",
      "epoch 20 loss 0.5491146445274353\n",
      "epoch 21 loss 0.5492742657661438\n",
      "epoch 22 loss 0.5491925477981567\n",
      "epoch 23 loss 0.5492414832115173\n",
      "epoch 24 loss 0.5493940114974976\n",
      "epoch 25 loss 0.5492187738418579\n",
      "epoch 26 loss 0.5492393374443054\n",
      "epoch 27 loss 0.5492630004882812\n",
      "epoch 28 loss 0.5492633581161499\n",
      "epoch 29 loss 0.5492414236068726\n",
      "epoch 30 loss 0.5492785573005676\n",
      "epoch 31 loss 0.5494145750999451\n",
      "epoch 32 loss 0.5493809580802917\n",
      "epoch 33 loss 0.5492773056030273\n",
      "epoch 34 loss 0.5492327809333801\n",
      "epoch 35 loss 0.5493472218513489\n",
      "epoch 36 loss 0.5492993593215942\n",
      "epoch 37 loss 0.5492441058158875\n",
      "epoch 38 loss 0.5493485927581787\n",
      "epoch 39 loss 0.5493019223213196\n",
      "epoch 40 loss 0.5491818785667419\n",
      "epoch 41 loss 0.5493572354316711\n",
      "epoch 42 loss 0.5493019819259644\n",
      "epoch 43 loss 0.5491770505905151\n",
      "epoch 44 loss 0.5491326451301575\n",
      "epoch 45 loss 0.5493165254592896\n",
      "epoch 46 loss 0.5492095947265625\n",
      "epoch 47 loss 0.5491846203804016\n",
      "epoch 48 loss 0.54909348487854\n",
      "epoch 49 loss 0.549290657043457\n",
      "epoch 50 loss 0.5492693185806274\n",
      "epoch 51 loss 0.5492672324180603\n",
      "epoch 52 loss 0.5492755770683289\n",
      "epoch 53 loss 0.5492969751358032\n",
      "epoch 54 loss 0.5490920543670654\n",
      "epoch 55 loss 0.5491763353347778\n",
      "epoch 56 loss 0.5491985082626343\n",
      "epoch 57 loss 0.5492417216300964\n",
      "epoch 58 loss 0.5492643713951111\n",
      "epoch 59 loss 0.5492880940437317\n",
      "epoch 60 loss 0.5493088364601135\n",
      "epoch 61 loss 0.5493596792221069\n",
      "epoch 62 loss 0.5493089556694031\n",
      "epoch 63 loss 0.5493008494377136\n",
      "epoch 64 loss 0.5492860078811646\n",
      "epoch 65 loss 0.5492746829986572\n",
      "epoch 66 loss 0.549253523349762\n",
      "epoch 67 loss 0.5493137836456299\n",
      "epoch 68 loss 0.5492423176765442\n",
      "epoch 69 loss 0.5492308735847473\n",
      "epoch 70 loss 0.5492091774940491\n",
      "epoch 71 loss 0.5491462349891663\n",
      "epoch 72 loss 0.5491243600845337\n",
      "epoch 73 loss 0.5491767525672913\n",
      "epoch 74 loss 0.5491548776626587\n",
      "epoch 75 loss 0.5490915775299072\n",
      "epoch 76 loss 0.5492990612983704\n",
      "epoch 77 loss 0.5492756962776184\n",
      "epoch 78 loss 0.5492135882377625\n",
      "epoch 79 loss 0.5492575168609619\n",
      "epoch 80 loss 0.5492421388626099\n",
      "epoch 81 loss 0.5491700172424316\n",
      "epoch 82 loss 0.5491466522216797\n",
      "epoch 83 loss 0.5491290092468262\n",
      "epoch 84 loss 0.5491268634796143\n",
      "epoch 85 loss 0.549103319644928\n",
      "epoch 86 loss 0.549083411693573\n",
      "epoch 87 loss 0.5492386221885681\n",
      "epoch 88 loss 0.5492150783538818\n",
      "epoch 89 loss 0.5492684245109558\n",
      "epoch 90 loss 0.5491947531700134\n",
      "epoch 91 loss 0.5491707921028137\n",
      "epoch 92 loss 0.5491493344306946\n",
      "epoch 93 loss 0.5490754842758179\n",
      "epoch 94 loss 0.5491174459457397\n",
      "epoch 95 loss 0.5490947365760803\n",
      "epoch 96 loss 0.5492538809776306\n",
      "epoch 97 loss 0.5492280721664429\n",
      "epoch 98 loss 0.5491984486579895\n",
      "epoch 99 loss 0.5491959452629089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        #start = i*batch\n",
    "        #end = start + batch\n",
    "        #x = X[start:end]\n",
    "        #y = Y[start:end]\n",
    "        x,y = HRdataset[i*batch:i*batch+batch]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch',epoch,'loss',loss_fn(model(X),Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_ds = TensorDataset(X,Y)\n",
    "HR_dl = DataLoader(HR_ds,batch_size=batch,shuffle=True)   #shuffle代表是否乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ,optim = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch  0  loss  0.539408802986145\n",
      " epoch  1  loss  0.4217658042907715\n",
      " epoch  2  loss  0.3445434868335724\n",
      " epoch  3  loss  0.32107123732566833\n",
      " epoch  4  loss  0.33560508489608765\n",
      " epoch  5  loss  0.2963852882385254\n",
      " epoch  6  loss  0.38604986667633057\n",
      " epoch  7  loss  0.26753678917884827\n",
      " epoch  8  loss  0.26621219515800476\n",
      " epoch  9  loss  0.2792888879776001\n",
      " epoch  10  loss  0.25702884793281555\n",
      " epoch  11  loss  0.2834628224372864\n",
      " epoch  12  loss  0.2485664188861847\n",
      " epoch  13  loss  0.24139216542243958\n",
      " epoch  14  loss  0.24129919707775116\n",
      " epoch  15  loss  0.23803061246871948\n",
      " epoch  16  loss  0.23438630998134613\n",
      " epoch  17  loss  0.2652568221092224\n",
      " epoch  18  loss  0.22176623344421387\n",
      " epoch  19  loss  0.2260322868824005\n",
      " epoch  20  loss  0.21701645851135254\n",
      " epoch  21  loss  0.22693881392478943\n",
      " epoch  22  loss  0.20940974354743958\n",
      " epoch  23  loss  0.25319334864616394\n",
      " epoch  24  loss  0.21331121027469635\n",
      " epoch  25  loss  0.20442485809326172\n",
      " epoch  26  loss  0.21174690127372742\n",
      " epoch  27  loss  0.20338359475135803\n",
      " epoch  28  loss  0.3306943476200104\n",
      " epoch  29  loss  0.19143997132778168\n",
      " epoch  30  loss  0.2690552771091461\n",
      " epoch  31  loss  0.19795788824558258\n",
      " epoch  32  loss  0.18166884779930115\n",
      " epoch  33  loss  0.20366807281970978\n",
      " epoch  34  loss  0.24237960577011108\n",
      " epoch  35  loss  0.19674792885780334\n",
      " epoch  36  loss  0.17843803763389587\n",
      " epoch  37  loss  0.17968149483203888\n",
      " epoch  38  loss  0.21140314638614655\n",
      " epoch  39  loss  0.20816992223262787\n",
      " epoch  40  loss  0.16850844025611877\n",
      " epoch  41  loss  0.1874762624502182\n",
      " epoch  42  loss  0.1866070181131363\n",
      " epoch  43  loss  0.16259801387786865\n",
      " epoch  44  loss  0.1771850734949112\n",
      " epoch  45  loss  0.1945147067308426\n",
      " epoch  46  loss  0.16023096442222595\n",
      " epoch  47  loss  0.16980639100074768\n",
      " epoch  48  loss  0.17533329129219055\n",
      " epoch  49  loss  0.15767541527748108\n",
      " epoch  50  loss  0.15643717348575592\n",
      " epoch  51  loss  0.1613941490650177\n",
      " epoch  52  loss  0.15553730726242065\n",
      " epoch  53  loss  0.17630571126937866\n",
      " epoch  54  loss  0.16708990931510925\n",
      " epoch  55  loss  0.1582443118095398\n",
      " epoch  56  loss  0.1695515662431717\n",
      " epoch  57  loss  0.17814382910728455\n",
      " epoch  58  loss  0.14973774552345276\n",
      " epoch  59  loss  0.15292248129844666\n",
      " epoch  60  loss  0.17833316326141357\n",
      " epoch  61  loss  0.16819866001605988\n",
      " epoch  62  loss  0.15635628998279572\n",
      " epoch  63  loss  0.14763693511486053\n",
      " epoch  64  loss  0.1750086098909378\n",
      " epoch  65  loss  0.17106689512729645\n",
      " epoch  66  loss  0.20671947300434113\n",
      " epoch  67  loss  0.15848250687122345\n",
      " epoch  68  loss  0.13129113614559174\n",
      " epoch  69  loss  0.13959984481334686\n",
      " epoch  70  loss  0.15570752322673798\n",
      " epoch  71  loss  0.1473167985677719\n",
      " epoch  72  loss  0.165305033326149\n",
      " epoch  73  loss  0.1361086666584015\n",
      " epoch  74  loss  0.12466201186180115\n",
      " epoch  75  loss  0.12186878174543381\n",
      " epoch  76  loss  0.15444695949554443\n",
      " epoch  77  loss  0.1314356029033661\n",
      " epoch  78  loss  0.12188450247049332\n",
      " epoch  79  loss  0.13274723291397095\n",
      " epoch  80  loss  0.14837472140789032\n",
      " epoch  81  loss  0.12318281084299088\n",
      " epoch  82  loss  0.12959825992584229\n",
      " epoch  83  loss  0.11112013459205627\n",
      " epoch  84  loss  0.1213761568069458\n",
      " epoch  85  loss  0.10794355720281601\n",
      " epoch  86  loss  0.11713568866252899\n",
      " epoch  87  loss  0.11473220586776733\n",
      " epoch  88  loss  0.12792611122131348\n",
      " epoch  89  loss  0.125139057636261\n",
      " epoch  90  loss  0.11771896481513977\n",
      " epoch  91  loss  0.12391125410795212\n",
      " epoch  92  loss  0.11261072009801865\n",
      " epoch  93  loss  0.1101829931139946\n",
      " epoch  94  loss  0.11330585181713104\n",
      " epoch  95  loss  0.11479001492261887\n",
      " epoch  96  loss  0.11743052303791046\n",
      " epoch  97  loss  0.10899890959262848\n",
      " epoch  98  loss  0.10432615131139755\n",
      " epoch  99  loss  0.10959040373563766\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x,y in HR_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        print(' epoch ',epoch,' loss ',loss_fn(model(X),Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加验证  \n",
    "## 了解过拟合与欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.mirrors.ustc.edu.cn/simple\n",
      "Requirement already satisfied: sklearn in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda\\envs\\pytorch\\lib\\site-packages (from scikit-learn->sklearn) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(X_data,Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 20)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11249, 20), (3750, 20))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(train_x).type(torch.float32)\n",
    "train_y = torch.from_numpy(train_y).type(torch.float32)\n",
    "test_x = torch.from_numpy(test_x).type(torch.float32)\n",
    "test_y = torch.from_numpy(test_y).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_x,train_y)\n",
    "train_dl = DataLoader(train_ds,batch_size=batch,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_x,test_y)\n",
    "test_dl = DataLoader(test_ds,batch_size=batch)  #测试集不需要乱序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何计算正确率？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = (y_pred > 0.5).type(torch.int32)\n",
    "(y_pred == labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y_true):\n",
    "    y_pred = (y_pred > 0.5).type(torch.float32)\n",
    "    acc = (y_pred == y_true).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ,optim = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch  0  loss  0.096  accuracy  0.971  test_loss  0.127  test_accuracy  0.958\n",
      " epoch  1  loss  0.124  accuracy  0.961  test_loss  0.157  test_accuracy  0.951\n",
      " epoch  2  loss  0.1  accuracy  0.969  test_loss  0.131  test_accuracy  0.96\n",
      " epoch  3  loss  0.093  accuracy  0.971  test_loss  0.123  test_accuracy  0.962\n",
      " epoch  4  loss  0.092  accuracy  0.972  test_loss  0.12  test_accuracy  0.962\n",
      " epoch  5  loss  0.098  accuracy  0.972  test_loss  0.126  test_accuracy  0.959\n",
      " epoch  6  loss  0.107  accuracy  0.967  test_loss  0.139  test_accuracy  0.959\n",
      " epoch  7  loss  0.101  accuracy  0.968  test_loss  0.129  test_accuracy  0.959\n",
      " epoch  8  loss  0.094  accuracy  0.973  test_loss  0.127  test_accuracy  0.963\n",
      " epoch  9  loss  0.088  accuracy  0.973  test_loss  0.12  test_accuracy  0.963\n",
      " epoch  10  loss  0.093  accuracy  0.972  test_loss  0.128  test_accuracy  0.962\n",
      " epoch  11  loss  0.096  accuracy  0.971  test_loss  0.123  test_accuracy  0.963\n",
      " epoch  12  loss  0.11  accuracy  0.964  test_loss  0.133  test_accuracy  0.956\n",
      " epoch  13  loss  0.093  accuracy  0.972  test_loss  0.125  test_accuracy  0.961\n",
      " epoch  14  loss  0.092  accuracy  0.973  test_loss  0.122  test_accuracy  0.963\n",
      " epoch  15  loss  0.096  accuracy  0.97  test_loss  0.128  test_accuracy  0.961\n",
      " epoch  16  loss  0.216  accuracy  0.906  test_loss  0.251  test_accuracy  0.9\n",
      " epoch  17  loss  0.099  accuracy  0.97  test_loss  0.129  test_accuracy  0.961\n",
      " epoch  18  loss  0.093  accuracy  0.972  test_loss  0.124  test_accuracy  0.962\n",
      " epoch  19  loss  0.09  accuracy  0.973  test_loss  0.121  test_accuracy  0.963\n",
      " epoch  20  loss  0.096  accuracy  0.971  test_loss  0.129  test_accuracy  0.96\n",
      " epoch  21  loss  0.098  accuracy  0.971  test_loss  0.131  test_accuracy  0.958\n",
      " epoch  22  loss  0.092  accuracy  0.972  test_loss  0.124  test_accuracy  0.962\n",
      " epoch  23  loss  0.095  accuracy  0.971  test_loss  0.124  test_accuracy  0.96\n",
      " epoch  24  loss  0.116  accuracy  0.962  test_loss  0.142  test_accuracy  0.952\n",
      " epoch  25  loss  0.119  accuracy  0.963  test_loss  0.151  test_accuracy  0.954\n",
      " epoch  26  loss  0.095  accuracy  0.971  test_loss  0.124  test_accuracy  0.962\n",
      " epoch  27  loss  0.124  accuracy  0.959  test_loss  0.156  test_accuracy  0.948\n",
      " epoch  28  loss  0.097  accuracy  0.97  test_loss  0.128  test_accuracy  0.961\n",
      " epoch  29  loss  0.087  accuracy  0.973  test_loss  0.12  test_accuracy  0.965\n",
      " epoch  30  loss  0.089  accuracy  0.973  test_loss  0.12  test_accuracy  0.964\n",
      " epoch  31  loss  0.097  accuracy  0.971  test_loss  0.128  test_accuracy  0.962\n",
      " epoch  32  loss  0.102  accuracy  0.968  test_loss  0.135  test_accuracy  0.957\n",
      " epoch  33  loss  0.092  accuracy  0.971  test_loss  0.125  test_accuracy  0.962\n",
      " epoch  34  loss  0.092  accuracy  0.97  test_loss  0.125  test_accuracy  0.962\n",
      " epoch  35  loss  0.109  accuracy  0.964  test_loss  0.141  test_accuracy  0.953\n",
      " epoch  36  loss  0.092  accuracy  0.971  test_loss  0.122  test_accuracy  0.961\n",
      " epoch  37  loss  0.101  accuracy  0.971  test_loss  0.129  test_accuracy  0.961\n",
      " epoch  38  loss  0.1  accuracy  0.968  test_loss  0.133  test_accuracy  0.961\n",
      " epoch  39  loss  0.094  accuracy  0.971  test_loss  0.127  test_accuracy  0.961\n",
      " epoch  40  loss  0.095  accuracy  0.971  test_loss  0.124  test_accuracy  0.96\n",
      " epoch  41  loss  0.096  accuracy  0.971  test_loss  0.128  test_accuracy  0.959\n",
      " epoch  42  loss  0.108  accuracy  0.965  test_loss  0.141  test_accuracy  0.957\n",
      " epoch  43  loss  0.098  accuracy  0.971  test_loss  0.129  test_accuracy  0.959\n",
      " epoch  44  loss  0.087  accuracy  0.974  test_loss  0.122  test_accuracy  0.962\n",
      " epoch  45  loss  0.101  accuracy  0.97  test_loss  0.133  test_accuracy  0.958\n",
      " epoch  46  loss  0.09  accuracy  0.973  test_loss  0.12  test_accuracy  0.962\n",
      " epoch  47  loss  0.09  accuracy  0.971  test_loss  0.124  test_accuracy  0.962\n",
      " epoch  48  loss  0.093  accuracy  0.972  test_loss  0.128  test_accuracy  0.961\n",
      " epoch  49  loss  0.1  accuracy  0.968  test_loss  0.132  test_accuracy  0.959\n",
      " epoch  50  loss  0.085  accuracy  0.974  test_loss  0.12  test_accuracy  0.963\n",
      " epoch  51  loss  0.102  accuracy  0.967  test_loss  0.13  test_accuracy  0.959\n",
      " epoch  52  loss  0.105  accuracy  0.966  test_loss  0.145  test_accuracy  0.957\n",
      " epoch  53  loss  0.083  accuracy  0.975  test_loss  0.119  test_accuracy  0.965\n",
      " epoch  54  loss  0.091  accuracy  0.972  test_loss  0.125  test_accuracy  0.962\n",
      " epoch  55  loss  0.091  accuracy  0.974  test_loss  0.125  test_accuracy  0.962\n",
      " epoch  56  loss  0.094  accuracy  0.973  test_loss  0.123  test_accuracy  0.962\n",
      " epoch  57  loss  0.115  accuracy  0.963  test_loss  0.145  test_accuracy  0.955\n",
      " epoch  58  loss  0.099  accuracy  0.969  test_loss  0.134  test_accuracy  0.956\n",
      " epoch  59  loss  0.1  accuracy  0.968  test_loss  0.135  test_accuracy  0.957\n",
      " epoch  60  loss  0.099  accuracy  0.968  test_loss  0.133  test_accuracy  0.956\n",
      " epoch  61  loss  0.096  accuracy  0.971  test_loss  0.13  test_accuracy  0.962\n",
      " epoch  62  loss  0.106  accuracy  0.967  test_loss  0.143  test_accuracy  0.955\n",
      " epoch  63  loss  0.098  accuracy  0.97  test_loss  0.137  test_accuracy  0.958\n",
      " epoch  64  loss  0.089  accuracy  0.971  test_loss  0.123  test_accuracy  0.963\n",
      " epoch  65  loss  0.118  accuracy  0.962  test_loss  0.147  test_accuracy  0.951\n",
      " epoch  66  loss  0.088  accuracy  0.973  test_loss  0.127  test_accuracy  0.961\n",
      " epoch  67  loss  0.092  accuracy  0.973  test_loss  0.127  test_accuracy  0.963\n",
      " epoch  68  loss  0.087  accuracy  0.972  test_loss  0.124  test_accuracy  0.963\n",
      " epoch  69  loss  0.088  accuracy  0.973  test_loss  0.128  test_accuracy  0.962\n",
      " epoch  70  loss  0.089  accuracy  0.973  test_loss  0.122  test_accuracy  0.962\n",
      " epoch  71  loss  0.092  accuracy  0.972  test_loss  0.13  test_accuracy  0.962\n",
      " epoch  72  loss  0.088  accuracy  0.973  test_loss  0.123  test_accuracy  0.965\n",
      " epoch  73  loss  0.104  accuracy  0.968  test_loss  0.139  test_accuracy  0.957\n",
      " epoch  74  loss  0.084  accuracy  0.975  test_loss  0.12  test_accuracy  0.964\n",
      " epoch  75  loss  0.085  accuracy  0.974  test_loss  0.122  test_accuracy  0.964\n",
      " epoch  76  loss  0.084  accuracy  0.973  test_loss  0.121  test_accuracy  0.964\n",
      " epoch  77  loss  0.09  accuracy  0.972  test_loss  0.13  test_accuracy  0.963\n",
      " epoch  78  loss  0.086  accuracy  0.974  test_loss  0.124  test_accuracy  0.962\n",
      " epoch  79  loss  0.108  accuracy  0.964  test_loss  0.148  test_accuracy  0.954\n",
      " epoch  80  loss  0.085  accuracy  0.973  test_loss  0.123  test_accuracy  0.961\n",
      " epoch  81  loss  0.097  accuracy  0.97  test_loss  0.131  test_accuracy  0.958\n",
      " epoch  82  loss  0.083  accuracy  0.975  test_loss  0.119  test_accuracy  0.963\n",
      " epoch  83  loss  0.124  accuracy  0.962  test_loss  0.161  test_accuracy  0.95\n",
      " epoch  84  loss  0.094  accuracy  0.971  test_loss  0.13  test_accuracy  0.963\n",
      " epoch  85  loss  0.098  accuracy  0.969  test_loss  0.132  test_accuracy  0.958\n",
      " epoch  86  loss  0.083  accuracy  0.974  test_loss  0.122  test_accuracy  0.965\n",
      " epoch  87  loss  0.086  accuracy  0.975  test_loss  0.123  test_accuracy  0.963\n",
      " epoch  88  loss  0.084  accuracy  0.973  test_loss  0.12  test_accuracy  0.962\n",
      " epoch  89  loss  0.083  accuracy  0.974  test_loss  0.122  test_accuracy  0.962\n",
      " epoch  90  loss  0.09  accuracy  0.972  test_loss  0.125  test_accuracy  0.963\n",
      " epoch  91  loss  0.091  accuracy  0.972  test_loss  0.128  test_accuracy  0.958\n",
      " epoch  92  loss  0.095  accuracy  0.969  test_loss  0.134  test_accuracy  0.959\n",
      " epoch  93  loss  0.095  accuracy  0.969  test_loss  0.135  test_accuracy  0.961\n",
      " epoch  94  loss  0.088  accuracy  0.975  test_loss  0.125  test_accuracy  0.963\n",
      " epoch  95  loss  0.082  accuracy  0.975  test_loss  0.121  test_accuracy  0.962\n",
      " epoch  96  loss  0.09  accuracy  0.972  test_loss  0.133  test_accuracy  0.961\n",
      " epoch  97  loss  0.082  accuracy  0.975  test_loss  0.122  test_accuracy  0.964\n",
      " epoch  98  loss  0.08  accuracy  0.975  test_loss  0.118  test_accuracy  0.965\n",
      " epoch  99  loss  0.081  accuracy  0.974  test_loss  0.12  test_accuracy  0.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x,y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        epoch_accuracy = accuracy(model(train_x),train_y)\n",
    "        epoch_loss = loss_fn(model(train_x),train_y).data\n",
    "        epoch_test_accuracy = accuracy(model(test_x),test_y)\n",
    "        epoch_test_loss = loss_fn(model(test_x),test_y).data\n",
    "        \n",
    "        Epoch.append(epoch)\n",
    "        train_acc.append(epoch_accuracy)\n",
    "        train_loss.append(epoch_loss)\n",
    "        test_acc.append(epoch_test_accuracy)\n",
    "        test_loss.append(epoch_test_accuracy)\n",
    "        \n",
    "        print(' epoch ',epoch,\n",
    "              ' loss ',round(epoch_loss.item(),3),\n",
    "              ' accuracy ',round(epoch_accuracy.item(),3),\n",
    "              ' test_loss ',round(epoch_test_loss.item(),3),\n",
    "              ' test_accuracy ',round(epoch_test_accuracy.item(),3)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABGMklEQVR4nO2deZxd8/nH389MMtkjq8geS4JIIwi1U9rat1iDqlb5tehCaamWNqq6aBVVRKtES0paqqRFQ6gKEpGFLCSxZCMR2RfJzDy/P577nXPunTsz507mznLv83697uuc8z3b95yZ83y+z/PdRFVxHMdxnCSUNHUGHMdxnJaDi4bjOI6TGBcNx3EcJzEuGo7jOE5iXDQcx3GcxLhoOI7jOIlx0XCcZoiI3CciK0TkzRr2i4jcLiILRGSWiOzb2Hl0ipNWTZ2BhqJHjx46aNCgps6G08x4/fXXP1bVnk2dj3pwP/A7YFwN+48DBqd+nwXuSi1rxL8RJxu5fiMFIxqDBg1i2rRpTZ0Np5khIu83dR7qg6q+KCKDajnkFGCcWu/cV0Ski4j0VtXlNZ3g34iTjVy/EQ9POU7LpC+wOLa9JJWWhohcIiLTRGTaypUrGy1zTuHiouE4BYyqjlXVkao6smfPlhilc5obLhqO0zJZCvSPbfdLpTlOXnHRcJyWyRPABalWVAcCa2urz3CchqJgKsIdp5AQkYeBI4EeIrIEuAFoDaCqdwMTgeOBBcAm4CtNk1On2HDRcJxmiKqOrmO/Apc1UnYcpwoPTzmO4ziJcdFwWjbTp8NLLzV1LgqeykpYvhwqKmw7c+62bdtqPi8X5s6Fhx+GKVOie+WD9eth61ZbV4Unn4Sl9WhGEH8PFRWwejV8+mmUVlkJf/ubPY+q/bs+/jhMmGBpmzZlv255efryrbfgnnvg44/T7535d6ioSH+u55+HBx6Ad97J/dlqRFUL4rfffvup00LYtq162tq1qj/9qeq6dcmv8/bbqp06qe6wg+r69VkPAaZpM/j/bA6/mr6RxYttOWuW6je/qbrffqqf+Yzq44+rTpmi+u9/qx58sJmoNm1U997bXvmJJ6rOm6d6/PGqJSWqhx1m2x98oHr++ap9+qiKqB55pOqLL6pOnKh6+OGq3/2u6sqVds+pU+3YtWtVf/Yz1dLSYArt/EmTVC+9VHXwYNVRo1Q3bFC96irV00+33wUXqP7kJ6p/+IPqjBmqFRWqmzer/va3qkOHqv7lL6oXXmjnDx+uetZZqjffrNqtm6VNm6Z65512v5IS1WHDVEeMsHyGPAamT7drLVtm+Tj1VNWDDlL99FPV665T7drVrtO+vb3Dfv1U+/e3tK5dVb/97ejZwq9VK9UvflH1k09Uf/Qj1bvvtjz26aP63nuqO+1kz9ivX5THIUNU99hDtUMHe8YZM1TffFP15z9X7d3b/kYHHGB/w3Cfjh3t/TfEN9Lk/8j5/iCKnvJy1ZNOsq8vn3z8sepRR6k+9FDtxz39tP23f/JJlFZZqXrmmfbvOHZszefee6/qwIH2RWzYoHrggXYtUL3jjqynuGjU/o385S9mZCZMMMPStq3qF75gRilu3Dp1Ur3xRtUrr1Q95hjVk0+29LIyM4jf+Ib9KS64QPUXv7B9o0ebgd9xx+g6/fqZoTzuONVVq1QHDLD0K64wgTn1VDOC48er7r57dN7RR9ty771tueeeqnvtFRnT8OvSRbVzZ1vv0UOrDO2oUaonnKDat6+l7bOPrYvYMxx9tOr119uncuyxlnbMMar33GPPcOmldm0w8Ykb5M9/3pannaZ6662ql19uaV/+suoZZ9j7KCuzY845R/X1102gH39c9XvfM6EcNCj9OUIew3qbNqqPPqp6ww32LGecYfeJv1tQ/dzn7F0efbR9Hn/8o73PffZR7d49e5nMRcNRXb3avkhVM+ageskl+b3nf/4T/efec4+lrVyp+vWvq27cGB0XLMrcuVHaI49EX/fpp1ueQ/7jnH56dI9nnrHlT35ixb3ddrNiZgYuGrV/I6tWmQ6HEvLChZa+ebMJyj//qfrCC6offlj9z/Gtb1lpfd482z7wQBOcK66wawU2bFC96y4zkBs2qN52m92vZ08TkGAwW7VSXbo0Om/5ctUjjlC95RbbPvFEO+7MM9PzsWWL6jvvqI4bp3rxxfabNMn+7a65xv5VAhUVqvPnm3fwySeqV1+tuv/+qkuWpF/z9ts1zWB362aG9+677d90113N6A8ZYseceKKVfWri7rvNI9uwofq+MWO0SlAmTrRPacQIS/vsZ+25avISPvjAhGrcOPNMamLdOvOqsuGi4ZhxPeYYW3/3XfszjxyZ33v+9a9aVSQNBnzsWEt76aXouO98x9Jefz1K+8EP7Ev8ylcs7jF4sBVFMzn88KioeNlltpwwwe596KFmZTJw0aj7G3ntNXvtv/991t21EjeUp5xiJfBzz1XdeeeazykvNwN6yCFm3O+9V6s8k9p47z0r+2QTsHwwZ47dM7MssnRpFGH9299M9BYtqv99ystV//GP9LLV3XfbO7nvvvpfNym5fiPe5LYlsnkznHEG/PSnsM8+1fcvXAhbttj6+vW2nDULxo2Dxx6D22+H/v2rn7c9fPKJLa+/Hq6+Gp57DmbMsLSNG6PjPvwweobAxx9Djx5w3HHwpz/B2rV2PVUQiY5buRL239+Of+wxS9tzT/uddVbDPk8Rsf/+9mpbt8793Pifp1cvq9zdcUdbr4nSUnjqqWj7wAPhv/+Fa6+t/V4DB1plcGOx557Z0/v0idZHjYLTTkt/D7lSWgonn5ye9tWvQufOcOaZ9b9uvvDWU43Jgw/CYYfVr5lGnOnTYeJE+2Xj44+tGQdEorF1K1x2mTXdGDkS1q2r371V7RoXXwzvvhulr1ply4svhu7d7esOorFhQ3Tc8lSn5XizkZUrTTQ+/3lo1Qo6dLDrBYGJH7fTTjBkCCxbZl/bbrtt3xfrAPUTjEx22sn+9ZYts/WktG9vLXz22GP789AU5OPfr3VrGD3aPofmhosGWHvBKVPgo4+q7ysvt//oTENfUQFHH20l66T885/WPPSww+pvtAHeeMOWixZZe76f/SzapxqJhmokGmDG+/jjYcWKdINfF5s2wWuvwZo15qWcdhr84Q/wq1/BlVfCeeeZZ9ChA+ywg/23P/lkumg8/zzMnBkJQaZo9OwJXbva3+Ghhyx99uzomIoKE5KePaMi4G67QVlZ8udw8kqvXtbE9J13avc0nJZN8YmGKrz6anoD8gkT4OCDrXh0441R4+dPP7XS74UXwi23pF/nD3+wEMy4cdUbS9fE229bifrdd82I1pdgjBctgjvvNOFasgRuvtlK8lu2mGexZUu6aHTsaJ4ApJf+a2PrVosLfPazFgq78UYTy7PPhj//2URk0iQz6N262TlnnGH3DsKwYQN84xvwve9FnkY8PBVEA8wLOuQQW4+LRhDBHj0i0agpfuA0CUEoystdNAqZ4hONP/3Jgqg//3mUFkrdZ51lBvh3v7PtV16BF16w9Tdjs25u2QLXXQdt28L775vx3rzZSt2L41McxKisNNE480zzOV95pf7PEETj3XctXxUVVvr/wQ/s+QKrV0eicdJJcNVVFnCG5KKxZo15LqNHm4eyapW9u699za5dUWHpH31kYSmAQw9NtxobNth1pk2LPKxsnkage3fo3TtdNMJcEHFPw0WjWRH/k7toFC7FJxr33WfLH/3IPA4wg9e5s3VFPeQQuOsuS1+zxpaf+Yx1yQzMnm3G84c/tO1Jk+D+++HWW600nY2lS01Y9t7bSuxTptQv/9u22f1btTLBCsY0zMg2fXp0bFw07rsPbrjBvA1ILhrhuGOPhWeeMQ9r5Eg46igz2v36mQcwZ07kaZSWmgB36GAB340bTSxCZTlEolFebvnMnOvhM5+x2tG77jJhiovGiBG2HpZOsyBej+GiUbgUl2i8/Tb8739WIq+shGeftfSPPrL/8pISOPdcG8tgzhxrxQMmJMuXR5W9oQR81lnQt69VSN92mxnL8eOjOoc48+fbcsgQ83SmTo3GCIhz6aUwdmy0vWwZPP10VKcyb56FjD73ueiY/fazpUjkhYCJXhCNTp1sGUQj3qIpG5Mmwf/9X3Rchw72Hi66yLZLSszL+f3vbfuDDyJPA6yeZdo0O2/16vRwFNh2ZaW9U9XqonHAAebBXXqp/c3iorH77vaOzzij9mdwGhX3NIqDvIqGiBwrIvNFZIGIXJNl/0ARmSQis0Rksoj0S6V/TkRmxH5bROTU7c7Q+PFm7C67zCpdQ6Xshx9GxaRRo8z4PvpoFEo5+GBbBm9j9mxo1w522QVOPRX+8Q8ThTvusPRx4+y4LVss3LV5swkWmME78EArab/5JkyeDL/9bdSp8667zFjffrsJQL9+VsofMABuusnEDNLb6I0bZ17SZz5jhjYQPI3WraFNG0tL6ml8/vMmXsE7COfFKSkx0QwETyMcv8ceJhqhHiPOpk0mdt/6lm1nisZ110WiPmdOJBo9ethyxAi7v9Ns6NjR/v0ht9ZTTssib1+diJQCdwLHAUOB0SIyNOOwW4BxqjocGAPcDKCqz6vqCFUdARyFzRfwTM6ZmDvXYvkhZPPKKzBsmDW03mmnSDSCpwGWfvDB5j0ET+Ogg2wZF4299jLP4te/tqa0115rcf6ePaPmri+9ZBXHL75ootKxo8XqDzzQ9r/2mp1/xRVWURwvjd9+u5XUVeGRR8wAP/101MIrVBZ37WphonPOqd73IohG8DKgZtH48EMzxPPnR15RSAcz/tmIN1qPexrx+2VrYrx5s3lNjz9u20EMAm3bWoV7x4723sNIbZnHOc0GkUgs3NMoXPJZVDsAWKCqi1R1KzAeOCXjmKHAc6n157PsBzgD+Jeq1jAeZC1s3mxNPxcvNuM7bZrF46Fm0QBryrl8uYlGmzaw665W5xEqw2fPtlI92P7zz7dwTOvWZqBDSCjUiWzaZJ7GkCH2ZQ0aZEWyefNgwQI75p57Ii9ixx2tknvmTLv+qFEmUitWmPEUMfErK7NlaCieKRohPBUXjXbt7PhM0XjhBbvff/9rTYwD4R1l8zTARLK01NbjnkagY0cLsQVKS+1drl0btfIK18lEBIYOjTyNzp0jj8lplvTqZf9iNf27OC2ffIpGXyDelGhJKi3OTGBUav00oJOIZBZXzwEerlcO4qXqxYvN8ATR6N3bDOKnn1qJPO5Pd+9uYZl166zfgYiFlRYsMMO9YkUkGpnERSN4Khs3mjCFUnlJiQnInDkWTtp1V0tfuNCWBx1k8f4nn4TBg83Q7rij5X/lSstf69YWGjv11Oje/frZMhjvbJ6GiHkNmaIR6mHefx/+858oPYSWavI0Skujd1eTaIRrDBtm77Fjx+p9YrKJBphYBtGo6Rin2dC7t/28v2Xh0tRB4auAI0TkDeAIYClQNYq+iPQGPgM8ne1kEblERKaJyLSVIeYdJxjL9euj1kVxT2P5chMASPc0unc3Q79ihZVuwUq427ZF3kYuorFpk10vbnh3391K9Vu3WqUvRE1/Q/jqvfdMXMAM5iefWJ6D8fzrX62ZbyB4Gr17m2HOJhpg+zZssPqT0F8kVKC/9571ztplF9uuKzwFkRjWFJ4K3sSdd1qldrt21UWjprDT0KGWh7ffdtFoAfz4x3DvvU2dCyef5FM0lgLxeEm/VFoVqrpMVUep6j7Adam0NbFDzgIeU9WsU7yo6lhVHamqI3tmMyhxT2PaNCudDx9uaTvtZMY8hIfiohFKzO++a54GmHegGglBTQYsqWgMGRI1O80Ujf33j4pqu+9uy9C/Yu7cmu8dRKNHD6vryBaegkg0fvhDa4arGnka06fbeWFMq7rCUxCJRk2eRmDHHaFLFxs3Ij5ESJcuNY9jMXRolC+vz2j2DB9urbGdwiWfojEVGCwiO4tIGRZmeiJ+gIj0EJGQh2uB+zKuMZr6hqYgMtLr15vR2WuvKCYeQiozZ6ZvQ1RizhSNysqoJ3lN/ncunkYgiMZ770V52Xnn9OOCUCxcWLPxzBSN2jyNdets/5QpVopfscKeMdSr7LuvLesKT0HdnkYgeG1xT0Okdg9i2DBbtm5tw5U4jtOk5E00VLUcuBwLLc0FHlHVt0RkjIiE9qJHAvNF5G2gF3BTOF9EBmGeygv1zkRJSRS/X7EiivlDJBIhLJPN01i7NjJ0maJRU3PPjh2rV4Rv3Jjd0wAzoHvtZevB09hhh6i3czgueBoVFTUb2dD8tUcPK73XJhpLlpiHUV4Ov/mNpR92WHRMEI0PP7SWTKGyOxu1eRrxZw7vsn37qP/HNddYv5maGDAAnnjCPKxzzqn5OMdxGoW8jqGoqhOBiRlp18fWJwATajj3PapXnOdOMOJr10ahDqjuaWTWaQRq8jRqEo3gacRDWWvXWn1INk8jtMwqLbVKaDCDP3SojR+d6Wlkrsdp1w6+8hU44QQz9u++W7NoxIfoGDvW8nbqqdaKqqQkCuOtXJldDOJ86Ut27yBsmfeCqAI+5DNw/vnpf5dsnHRS7fsdx2k0muHAuw1Mp07maaxdGwkApHsaO+xgpelA3EhmikYYnLA20Sgvt8rfIBqhkj4uGjvsYBXWoRlu167WnLa01I67/HILzYS8xA1ybeGcMEzK3/9uIblsotGhQ5S3HXeMmiZvS1UdDRgQCadq3e0nBw6E7343+75wbufOUUivfftof/A+HMdpERS+aMQ9jbhoxL2JI49MP2d7PQ2I7gnZRQOsh3rwcLp1M9EITXwHDIALLoiO7drVBKW28FScrl2tf0RlZXZPI/Dww+ZV9OgRNQoYPNjqflq3ru4h5UpcNAIuGo7TYmnqJrf5p1Mnq88oL08XjbjRzxz2vH37aJ6GXOs0solGaNabaXwPPzwKP3Xtast4HuOUlEQV4ElaEfXubQITz1MgLhqDBkXX69/fBGu33dLP256eWtlEIx6e8l5gjtOiKHzRCJW+YHUFcX70IxsGPRjJgEjkbQQjLtLwnkacukQDIg8jiadxwgnV8xSIG+q4ALVpY6P1fvvb6efly9Po2NHHj3KcFkbhh6c6dYr6BGQa5DFjaj6vWzdrbprZTyNJ6ykw0Qitp8K4SbUZ31B3kSlscUK9RhLRiFcu1yQarVpV3xcPieXb0/DQlOO0OAq/mNexY2ToayvFZxI8jfqGp0JIDKJzGsrTSNrJLQyfHp+lECJD3r177eM95NvTcNFwnBZHcYhGIBfRCCX/+laEh5BYnCSeRm15HDTIWn0lHbTvj3+EL3zB6k6y5aMu8UkgGmvWZJ9avdq9ahGNTz9NH9HdcZzmS+GLRjz8Uh9Poz49wiESjbiBT+Jp1Bae+sEPbOympOy8s822lxnOinsatZEgPHXZZeljJlYjnBt/9yE8lbr+ffdZNC0+sZ/jOM2TwheNuMGrzSBn0lCeRu/eVbsemdSdE06I5nZKI0l4qnPnaCDBhFRU2HxNzz0XS8xVNGoRuxkz0qfL+Phj6zcYOsUnCU998IF5G2H23aQsWmQji4RZdx3HyT+FLxr19TT697dmt0Fo6isasUmK/jGpIxMnwumnR61hq0hSEV4PVq+Gf/4T/vznWGIw5EnDUzV4GhUV1rUjiOBrr1kEbfhwm2wQiAYjjPe4z6gID+0Fcpk2XdXmaHroIfs5jtM4FL5oBIMXelrXQLXpur/2tWiOa0jeI7yszH5ZPI2F77eibVubruKFzBG1knga9SC0+k0zyA3kaSxebB3f162zqb5PPNEaeH3rW/Dyy6l7du5s7/HCC6MTMzyNMNHhK68kfixWrLDxHcvKrJFb+LM4jpNfCl80guGLD2ORwb//bQX9MMArYKXh1JwZr70Ga8o7JvM0IL1vSBCNNm1Y9K5wyimmX2EaiyoGDrQdOYafsnHRRXD22bYeRGPevMg417dOY9GiaHxHsGk3wAz2yy9bd5Q777SpzLt0gVtvTR04fHjVMC1bt8KLC8z7mrV5MCtXRvl69dXqDb3iLF8Os2bZ+ty5tjzySJsAMDyn4zj5pfBFI1tFbAavv24x+G99ywzg449bHzcwY3TIIfDLOSemiYZKCb/4hY0pWI1OnaKJh1LhqfXte7FypU1Tsd9+MHlyxjkDBphVzBzSpB68+mpUPxA3pq+9llrZaSfzHsLoulmYORN+PuUI2+jQgd/+1jqvH3641T+AjageCGMthvmfvvY1G/4qdIYPjB8PR1yxL0vpw/GPXMiYMZForFsXCfdLL8FPf5p+7tVX2/3D9OIQzd0QRnB3HCe/FI9opOoKVKPSauCDD2w5aZKN23faaTZYLMDUqRa6emP1oLTOfT+7tR3XXBMrTccJJfRhw6pi+QvLbKjzXXYxXXj11WgOpip69kw8T+a8eVlCaqnne/99c3QqKtJFY8oUy/7sJV0tnlRV8VCdcePg2icOopxStENHbrrJQk/r11tobf78qLQP6QP0Anz5y3b/Rx9Nv26YMmQtO7B6S1sWL7Y6jUMPNeftwQftvZx3nnXYD2Mogs0TtXYtTJxo9+7QwearAhcNx2ksCl80ggFPeRq33w577x2NiA4mGnvvbYX9r389/fQQZ5+9pl+Vp/E+A/jhT9vSqlVU4q2ogF/+MlVqnj/fEr/xjar6gEWtbF6MXXe1PnfbtllI55VX4NJL4cYbk8flly0zPXr4YbvGpZfavYNIbNhg68uXR6LRvTtMmGAjkQ8fDu8tb1OrQIVO9Jtpx+KN3fj4Y7jiCosy/eAHsMceNltsIIhGqJoZNsyie5mV1MG4by7txOZtrVixwt7ZsGEwapSN0n7VVZGQh+M//TR6rQ89ZKKxxx5RO4NCEw0ROVZE5ovIAhG5Jsv+ASLyvIi8ISKzROT4psinU3wUvmjEwlMrVsD1qdk8grEHM1C77ALf/KYZ5IBqVIG8dFM3Vpd3gspKnuREwKaRWLrUwiozZ8L3v58amTxVPP7oi1/i0J98gWX0ZiG7AiYaoXQ8e7bNf3TXXZavbP0BszFnTtRy6Ve/gnvusXv/4Q+RsQ3PFUTj5pvhrbdMaCB9ttVAGO0kvn/T8IOYvtm8pEMPtXDQ66+bFlZWRkb7gw/MU4g3Vjv3XBO1eJ6CcV/70JOoCh99ZKLRtauJ0po19j5C5Cz8PebNs2ceMMC8wenTbZ6qUGVUSKIhIqXAncBxwFBgtIhkTjryQ2xis32wWTF/37i5dIqVwheNmKdx223RhHEhTKJqRm3AAIvDx43ehg3mCYRZVGd/OgQqK3mKE9ht10pOTs0/OG9e1Ffh+eex5lF//SvT3+nE/+Z2YyZ7s7ByEN26mcPTrZsN+7RyZXrMf906eOwxGxJrzBh45JHsjxQqoJcvt/t+/vMW67/uuvSK6rhoXHihHRcIzVwDCxZYJC30HQyisfGxZ5i+qEvVvEynnGLp48fDHXfAj39s2++/b88Wbx8Qn/r8pZdM7IJx/6SkZ1UeKypMNA46yMJ9TzwRNREO7zXMGXXnnVYOWLXKRKNzZ2uzUEiiARwALFDVRaq6FRgPnJJxjAKh88sOwDIcpxEo/AELY57Gf/5jhmnevGhm1TVrTBwGDLB4/LRpZrSuvtrqMz75xDyQn/wEZm/dnZFbSniez/F/x1Wy555mIefOtcpZgP/+F8ofP5pWrWBNqlS/mXYs2tqfXc3ZqJoWe+VK+7VrZ+evX2/jBW7YEGW/c+fqVQ+hAjqIxrBhFqLaf3/49a+j44JotG9vXSX+8Q+rgD/hhOqi8c475jm8845V/IehQTZtslL90KF2na9+1Sry99vP9gfx/fDD6g2/QtuDNWvME9pll5hopHp/h3qZLl3svXznO7YdBgYOnsbs2fYMxxxj/U5OPx2OOMLO6dOn4ESjL7A4tr0E+GzGMT8GnhGRbwIdgM+TBRG5BLgEYMCAAQ2eUaf4yKunkSAuO1BEJqVispNFpF9s3wAReUZE5orInNSc4bnTpg3stBPr+u7J669bfcKgQZGxC6GT8D0NGRIZv6lTbfmFL0CXsk3M3rYHL7/Xhy2047hjlF13NUM2d25UIl63LirtB8O8hbasquhSbfK9FSvMOIb7rV5tgnHDDSYigwfbKOWhIVYgiMaSJWas+/Y1I967t1Xyl5WZwQ6iEYx3+/bR1N+ZohFE4pNP7H6rVtn2xo0mGuG8Vq0iwYD0jt6hPiMQKsXXrrXrvfVW5MFUNf+t4dzu3e3dxj2NPfe0tIMPNjE55BDb17t3wYlGEkYD96tqP+B44EERqfY9q+pYVR2pqiN7Jhkd2XHqIG+ikTAuewswTlWHA2OAm2P7xgG/UtU9MXc9o/Fm4ozA/Pm8tNf/UVFhLZdqEw2IDFhoHdS7N+ze9SMWVgxk/ZbWAOzUW2jVygz73LlmxEIfuNAHIxjmzbRjk7ZNm7Buxx3NgH78MVUeSChVd+liFc633WYCcckl6ZXkITw1Z05UryAStdbt39+6fWSKRrh2PG+BIBqrVqWHzD780AzysGFkJR7Oy+zMHu67erX9Pvggag1Vl2iUlNhzLVtm4aupU2HEiGh/vA6/AEVjKdA/tt0vlRbnIuARAFWdArQFEg5/7Dj1J5+eRpK47FAgjIr0fNifEpdWqvosgKpuUNXMBqrJ6dyZyf8tpawMDjzQROP996PmqVC7aPTqBWWlFVRoadS3r5W9uj33NOO9dKmFcHbcMZo1NS4amyvbpE1Y17OnhclUI08jlKqDsT3uOKszeOABq+QGM7qLFplRDR5I3762DKIxYEDNotG2rTlfmUY7CMWqVemV5MEYZxr1QOvW0aggmceE+y5dWn3YlMzBCbONntKnj507daqJ6zHHZM9DAYrGVGCwiOwsImVYRfcTGcd8ABwNICJ7YqKxslFz6RQl+RSNbHHZvhnHzARGpdZPAzqJSHdgCLBGRP6ealL4q5TnkoaIXCIi00Rk2sqVtX8vr71mIZb27U00tmyx5rf33WfhnHjoKAwDNXeuVYl06AAlolQikWiUWlF3771h4UJrDtqnjxnKMFhfmqdR0SbN0+jZMxqzKXgaQTTiIZ/rr7d7/PGPtv3uu2aAQ7gIohZMYfqMAQPs98EHlofMaSu6dKnd04gPdR4EJHOupjhBHDINf+vW9u6CVxcnUzSyiVLfvuZpPPWUiWRN3Up697Z3Wa3fSwtFVcuBy4GngblYK6m3RGSMiKSaX/Bd4GIRmQk8DFyo6oOpOPmnqVtPXQUcISJvAEdgLngFVkF/WGr//sAuwIWZJ+cSr129Ohozb9AgW37nO2aER41Kb/UTDNj69dZ5GlKioSXVRGPffc1beO89M96dO0ctlqrqNEo7srm8LM3TiItUpqcRN/Ii1tHt1VfNgwn1GYcdFh0TPI3ddrOOiSecYKOir15t+crsDJ9NNGryNIKA1DZfUtiXzfDvsEP9RSN4Gk89ZfUYQcwzueACq0cK07oXAqo6UVWHqOquqnpTKu16VX0itT5HVQ9R1b1VdYSqPtO0OXaKhXyKRp1xWVVdpqqjUm3Nr0ulrcG8khmp0FY58DiwL9vBunWRcdt5Z1t2725eQui7EOjY0YaBgrhokO5ppN5cvMTft6/dI3gQVZ7GBZewuaJ1NU8jMGCA3S+baACMHm3i8dBD1u+hVauo1F1aGgmQiA3dceaZ1vENTAySiEa8IjwuGkk8jdpEo0uXZKKRTZT69jXhfuMNOOmkmu/fp495Y60Kvy2g4zQ5+RSNOuOyItIj1uLjWuC+2LldRCSY1qOA+HCCiZg50ypPX345XTR22cVaSd16a/bSq0iUHryTkhKlUoXKCk1tW3rv3pGwBE8jUzTWte9NRYXU6Gn07GlCVZNo9Otn/TAeesgq2fff3yrgwe5dWi1wZ3UtgVw9jY8+iuaOykU0stVL7LBD1Hx2wAC7Tvv2UZ2KiB2T7RnCu99xR5vsyXGcpidvopEwLnskMF9E3gZ6AcENr8BCU5NEZDYgwL31ycfMmRYXj4tG27ZWB/GlL9V8Xig1p3ka8fBU7M0Fb6MmTyM0X83maYiYx9OpU2Rcs5W6zz3X8vzKK1bhHXpCx6brSGPgwKqBZesUjcrK6N4hPDVwoG03hKcRuOEGG4SwXbtINHr3rnkKkWOOsWFdZs3avmnKHcdpOPLq0KvqRGBiRtr1sfUJwIQazn0WGL499w9GbPly60RWW1y+pnOrRKNEs4anwERj4sSaPY0QislsPQVR7/C4Uc6WzzPOgMsvt9ZTn/ucCUKXLlF9RialpTYq7cyZdYvGqlVWub7TTiYS8+dbs93FixumTiNw2ml2zC9/GQnpQQdZo4Rs7LRT+vhWjuM0PU1dEZ5XghELzWpzEY1q4SmBSrJ7GqNHmycwZEgkGqpRaTqbaITwVFgG0RDJXqru1s2a4IbObWAV+bV5SyFEVZNohLY2ITQVjp8928Z+6tAh6ule3/BUSAthKEh/D/fea2NJOY7TMiho0QgV2qED33Z7Gprd0xg6FP7yl6gndnm5CUXoyBZEIx6e2mEHE4DgcQSj3KlTzfM73XqrDQUSROWGG6zlV03UJhpbt0Yl/OBNDE11vVS1EWpDflu3juo4spHE04iPSxUXjfi64zjNn4JubyJiBrI+nkbWOg1KqKxMrwjPJNwjPrJrCMXEDWQYfyrT06gtj7vsktvEfkE0Mj2AeK/wm2+2zoPx4yFdNGrzMiAShtrqNOJ5CO9BpHYxchyn+VHQogFmyOrjaVRvPVVzeCpONtEI/TbingbYiK39UqNtJRGNXDnxRPj5z6MxmgLBgE+ebFOzlpaal5Sa3RaIwlNJ8nTeeXZMtq4y2QQliEbbtonnnHIcp5lQFKIRhvXIxSAfeGA0CCCAiJpopIbDSCoarVtHYarMUMypp0brCWalzZl27Wx02UyCaHzpSyaOM2ZY3UXI3y67WH6Sehp9+1afvCrzXtlEw0NTjtPyKOg6DUgPi+RikI8/3oZJb23jE0aeRkVu4akQ3oLajWQ+PI2aCO+kosI8jb59rTd58K6CxxE8jbpEozayDTESmgK7aDhOy6PgRSNewt0eg1xVp5GjaARPBaqHp+I0pmjExfOii6L1du2smW6YrCmpp1Eb7mk4TmFRFOGpwHaJRgko1XuEZ1KbaDQXT2PXXa2J8FVXVe+JHUbeheR1GrWRzdOI12k4jtOyKHjRCMaqrGz7WupY577knsY771glb3P0NMrKrIlwTYTKafc0HMfJpGjCU9trjDPrNGpq9RPus2qV1RPEQ0HNxdNISkPUafTqBSefDEcdFaW5aDhOy6XgPY0GE42EdRpt2lhJfutWq1COG8bawjGh9VRzEo2G8DRatbIOiXG8ItxxWi7uaSQkaT+N+L3iotG2be3nNGdPo6Hz5J6G47RcCl40Qky94USjdk8jfq+4aNRlIDPHomoONISnkQ0XDcdpuXh4KiFRRXjYrvnYuGiEQQtrqwQHG+zwpZesU2FzoSHqNLLhouE4LZeC9zTyVRFel2i0a2dNW3OJ3x9ySPbJiJqKfHsa3uTWcVoeReNpbO/wHEkHLARrNVVWZgLQkkvV+arT8Ipwx2m5FLxohCG5t1s0MirCaxto7667qDouGMa6wlPNkZEjrXf48O2aCqs6LVlIHafYyWt4SkSOFZH5IrJARK7Jsn+giEwSkVkiMllE+sX2VYjIjNTvicxzk1JaanNrX3JJfa9gROEpECprFY2ysuql6ZZoIPv0gWefzT6P+vbQkt+J4xQ7efM0RKQUuBP4ArAEmCoiT6jqnNhhtwDjVPUBETkKuBkIc9FtVtURDZGXs8/e/muUlJpoaKVSQiVJ9bYlexr5wkXDcVou+fQ0DgAWqOoiVd0KjAdOyThmKPBcav35LPubDfHwVIlo4vPcQFbH34njtFzyKRp9gcWx7SWptDgzgTBh6WlAJxHpntpuKyLTROQVETk12w1E5JLUMdNWrlzZgFmvTppoUJn4PDeQ1fHWU47TcmnqJrdXAUeIyBvAEcBSINUTgoGqOhI4F/itiOyaebKqjlXVkao6sme2aeMakJISiYlGck8jGEYPT0XsthtcfDEcfXRT58RxnFzJZ+uppUD/2Ha/VFoVqrqMlKchIh2B01V1TWrf0tRykYhMBvYBFuYxv7US7xFeIu5pbA9lZTB2bFPnwnGc+pBPT2MqMFhEdhaRMuAcIK0VlIj0EJGQh2uB+1LpXUWkTTgGOASIV6A3OunhqdzrNNzTcBynEMibaKhqOXA58DQwF3hEVd8SkTEicnLqsCOB+SLyNtALuCmVvicwTURmYhXkP89oddXopFeE5+ZpdOlizVcdx3FaOnnt3KeqE4GJGWnXx9YnABOynPcy8Jl85i1X6tt6qrQU5syB7t3rPtZxHKe5U/A9whsK66dRSmWl5NR6CtJn73Mcx2nJNHXrqRZDSYl1Aa+olJw8DcdxnELCRSMhYYDCci3JqSLccRynkHDRSEhJqXka5ZWlOVWEO47jFBIuGgmp8jQqSzw85ThO0eKikZCS1ORI5eqi4ThO8VKnaIjISbEOeEVLVKdR6nUaTt6pa1qB1DFnicgcEXlLRB5q7Dw6xUkSMTgbeEdEfikie+Q7Q82V0HqqnFZep+Hkldi0AsdhI0GPFpGhGccMxkZROERV9wK+09j5dIqTOkVDVc8nGvfpfhGZkhpdtoFnjm7eVIWnaOXhKSffJJlW4GLgTlVdDaCqKxo5j06RkijspKrrsJ7b44He2DDm00Xkm3nMW7Mi3dNw0XDySpJpBYYAQ0Tkf6npA47NdqHGnD7AKQ6S1GmcLCKPAZOB1sABqnocsDfw3fxmr/lQ1eTWRcNpHrQCBmPjt40G7hWRLpkHNeb0AU5xkGQYkdOBW1X1xXiiqm4SkYvyk63mR1VFuIuGk3/qnFYA8z5eVdVtwLupQT8HY6NLO07eSBKe+jHwWtgQkXYiMghAVSflJ1vND/c0nEakzmkFgMcxLyNMHzAEWNSIeXSKlCSi8SikjdBXkUorKlw0nMYi4bQCTwOrRGQONn3A1aq6qmly7BQTScJTrVItOABQ1a2p0k9R4a2nnMYkwbQCClyZ+jlOo5HE01gZK90gIqcAH+cvS80Tbz3lOI6TzNP4OvAXEfkdIFhTwAvymqtmSNzTaOOi4ThOkZKkc99CVT0Q65m6p6oerKoLkly8rqEQRGSgiEwSkVkiMllE+mXs7ywiS1KC1aS4p+E4jpNw5j4ROQHYC2grYsZTVcfUcU4YCuELWPPAqSLyRMZc37cA41T1ARE5CrgZ+FJs/41AWlPfpsIrwp36ICIdgM2qWikiQ4A9gH+lmso6TosjSee+u7Hxp76JhafOBAYmuHaSoRCGAs+l1p+P7xeR/YBewDMJ7pV34qKR0k3HScKLWGGrL/a//CXg/ibNkeNsB0kqwg9W1QuA1ar6E+AgrE14XSQZCmEmMCq1fhrQSUS6p0bV/TVwVW03aMwhEtzTcOqJqOom7P/896p6Jua1O06LJIlobEktN4lIH2AbNv5UQ3AVcISIvAEcgfV6rQAuBSaq6pLaTm7MIRLSRSOvt3IKCxGRg4DzgKdSaaVNmB/H2S6S1Gn8MzWmza+A6YAC9yY4r86hEFR1GSlPQ0Q6Aqer6prUR3aYiFwKdATKRGSDqmadV6AxSBONEvc0nMR8BxvC/LFUB71dsFCs47RIahWNVJhokqquAf4mIk8CbVV1bYJrVw2FgInFOcC5GdfvAXyiqpXYh3UfgKqeFzvmQmBkUwoGeHjKqR+q+gLwAlR9Tx+r6reaNleOU39qDU+ljPmdse1PEwpG0qEQjgTmpwZb6wXclPsjNA4ennLqg4g8lGo63gF4E5gjIlc3db4cp74kCU9NEpHTgb+nhi5ITIKhECZg83TUdo37aQatTdJGufXwlJOcoaq6TkTOA/4FXAO8joV7HafFkaQi/P+wAQo/FZF1IrJeRNblOV/NDg9POfWktYi0Bk4Fnkj1z/B/IKfFUqenoapFNa1rTZS0CqLR2sNTTi7cA7yHNS9/UUQGAkVX6HIKhzpFQ0QOz5aeOSlToVNSGjllHp5ykqKqtwO3x5LeF5HPNVV+HGd7SVKnEa+0a4v19H4dOCovOWqmhPAU4J6GkxgR2QG4AQiFrxeAMUCiBiWO09xIEp46Kb4tIv2B3+YrQ82VNNFwT8NJzn1Yq6mzUttfAv5ENBKC47QoEg1YmMESYM+Gzkhzxz0Np57sqqqnx7Z/IiIzmiozjrO9JKnTuIOotUcJMALrGV5UlLTyOg2nXmwWkUNV9SUAETkE2NzEeXKcepPE05gWWy8HHlbV/+UpP80W9zScevJ1YFyqbgNgNfDlJsyP42wXSURjArBFVSvA5skQkfapkTuLBq/TcOqDqs4E9haRzqntdSLyHWBWk2bMcepJks59k4B2se12wH/yk53mS7poNGFGnBaJqq5T1dA/48omzYzjbAdJzF9bVd0QNlLr7fOXpeaJh6ecBsT/g5wWSxLR2Cgi+4aN1Ix6RVeR5xXhTgPi/0BOiyVJncZ3gEdFZBlWQtoJm/61qHBPw8kFEVlPdnEQ0sO9jtOiSNK5b6qI7AHsnkqanxp0rahI9zSaMCNOi8DHbHMKlTrNn4hcBnRQ1TdV9U2gY2pGvaLCK8Idx3GS1WlcnJq5DwBVXQ1cnLccNVO8TsNxHCeZaJSKSFUxW0RKgbL8Zal54p6G4zhOMtH4N/BXETlaRI4GHsZmIKsTETlWROaLyAIRqTbHt4gMFJFJIjJLRCaLSL9Y+nQRmSEib4nI13N5qHzgouE4jpOs9dT3gUuw4RDAerLuVNdJKY/kTuAL2CCHU0XkCVWdEzvsFmCcqj4gIkcBN2OjgC4HDlLVT0WkI/Bm6txlSR+sofGKcMdxnASehqpWAq9is48dgM2jMTfBtQ8AFqjqIlXdCowHTsk4ZijwXGr9+bBfVbeq6qep9DZJ8plv3NNwHMepxRiLyBARuUFE5gF3AB8AqOrnVPV3Ca7dF1gc216SSoszk2hegdOATiLSPXX//iIyK3WNX2TzMkTkEhGZJiLTVq5cmSBL9cdFw3Ecp/YS/DzMqzhRVQ9V1TuAiga+/1XAESLyBnAEsDTcQ1UXq+pwYDfgyyLSK/NkVR2rqiNVdWTPnj0bOGvpxIXCO/c5jlOs1CYao7C6hedF5N5UJXgu5nIp0D+23S+VVoWqLlPVUaq6D3BdKm1N5jHYzGeH5XDvBidNNNzTcBynSKnR/Knq46p6DrAHVt/wHWBHEblLRL6Y4NpTgcEisrOIlAHnAE/EDxCRHiIS8nAtNjUmItJPRNql1rsChwLzc3qyBsZFw3EcJ1lF+EZVfSg1V3g/4A2sRVVd55UDlwNPYxXnj6jqWyIyRkROTh12JDBfRN4GegE3pdL3BF4VkZnAC8Atqjo7t0drWFw0nMakrubqseNOFxEVkZGNmT+neMlpjvBUb/CxqV+S4ycCEzPSro+tT8Ameco871lgeC55yzdpolHadPlwCp+EzdURkU7At7HWjY7TKHiZOSHuaTiNSJLm6gA3Ar8AtjRm5pzixs1fQtJFw5tPOXmlzubqqTlu+qvqU7VdqDGbpTvFgYtGQtzTcJoLqcYjvwG+W9exjdks3SkO3PwlxEXDaUTqaq7eCRgGTBaR94ADgSe8MtxpDNz8JSQuFOLhKSe/1NpcXVXXqmoPVR2kqoOAV4CTVXVa02TXKSZcNBLinobTWCRsru44TUJOTW6LGYk5Fy4aTr6pq7l6RvqRjZEnxwH3NBLj/TQcx3FcNBLjTW4dx3FcNBLjnobjOI6LRmLc03Acx3HRSIx7Go7jOC4aiXFPw3Ecx0UjMWlNbktdNBzHKU5cNBIiAkIl4P00HMcpXtz85UCJi4bjOEWOm78cKBG1pYenHMcpUvIqGnVNWSkiA0VkkojMEpHJItIvlT5CRKaIyFupfWfnM59JqfI0XDQcxylS8iYasSkrjwOGAqNFZGjGYbcA41R1ODAGuDmVvgm4QFX3Ao4FfisiXfKV16R4eMpxnGInn+YvyZSVQ4HnUuvPh/2q+raqvpNaXwasAJp8BhkPTzmOU+zkUzTqnLISmAmMSq2fBnQSke7xA0TkAKAMWJinfCbGw1OO4xQ7TR1ouQo4QkTeAI7AZierCDtFpDfwIPAVVa3MPLmx5z92T8NxnGInn6JR15SVqOoyVR2lqvsA16XS1gCISGfgKeA6VX0l2w0ae/5j9zQcxyl28ikatU5ZCSAiPUQk5OFa4L5UehnwGFZJPiGPecyJyNNo4ow4juM0EXkTjYRTVh4JzBeRt4FewE2p9LOAw4ELRWRG6jciX3lNSgkp0fCxpxzHKVLyOt1rXVNWpryIap6Eqv4Z+HM+81YfSsTDU47jFDdNXRHeovCKcMdxih0XjRyoCk+5aDiOU6S4aOSAh6ccxyl2XDRywMNTjuMUOy4aOeCi4ThOseOikQNep+E4TrHjopED7mk4jlPsuGjkgFeEO45T7Lho5ECVp9HKX5vjOMWJW78c8PCU4zjFjotGDlSJhr81x3GKFDd/OeDhKcdxih23fjng4SnHcYodF40ccNFwHKfYcdHIAQ9POY5T7Lj1y4Ew95L4JEyOk51t22DLlqbOhZNHXDRywD0Nx6mDq66CY49t6lw4eSSv1k9EjhWR+SKyQESuybJ/oIhMEpFZIjJZRPrF9v1bRNaIyJP5zGMueJ2G01gk+HauFJE5qW9nkogMbIp8VmPhQvs5BUveRENESoE7geOAocBoERmacdgtwDhVHQ6MAW6O7fsV8KV85a8+lJS4p+Hkn4TfzhvAyNS3MwH4ZePmsgY2boQNG5o6F04eyaf1OwBYoKqLVHUrMB44JeOYocBzqfXn4/tVdRKwPo/5yxn3NJxGos5vR1WfV9VNqc1XgH40BzZssJ9qU+fEyRP5FI2+wOLY9pJUWpyZwKjU+mlAJxHpnsc8bReh/ttFw8kzSb6dOBcB/8q2Q0QuEZFpIjJt5cqVDZjFGtiwAcrLYevW/N/LaRKaOs5yFXCEiLwBHAEsBSqSntzYH4RXhDvNDRE5HxiJhXOroapjVXWkqo7s2bNn/jMUQlMeoipY8mn9lgL9Y9v9UmlVqOoyVR2lqvsA16XS1iS9QWN/EFV1Gu5pOPmlzm8HQEQ+j303J6vqp42Ut9rZuNGWLhoFSz5FYyowWER2FpEy4BzgifgBItJDREIergXuy2N+thv3NJxGIsm3sw9wDyYYK5ogj9lxT6PgyZv1U9Vy4HLgaWAu8IiqviUiY0Tk5NRhRwLzReRtoBdwUzhfRP4LPAocLSJLROSYfOU1KVV1Gi4aTh5J+O38CugIPCoiM0TkiRou13hs3Wqd+wDWN6s2LE4D0iqfF1fVicDEjLTrY+sTsOaC2c49LJ95qw8ennIaiwTfzucbPVN1Efcu3NMoWLzInANVnoa/NcepTqjPABeNAsbNXw5UeRr+1hynOu5pFAVu/nLAPQ3HqQUXjaLAzV8OuKfhOLXg4amiwM1fDpQMsJEaXDQcJwvuaRQFbv5yoKR7N1v6W3Oc6rhoFAVu/nIgiIWLhuNkwUWjKHDzlwMuGo5TC6FOo1s3F40Cxs1fDrhoOE4tBKHo1ct7hBcwbv5ywEXDcWphwwZo3do9jQLHzV8OuGg4BcnWrXDhhTB37vZdZ8MG6NjRfrmIxpYtcP758Pbb23f/QmLjRrj+eti0qe5jGxk3fzngouEUJNOnwwMP2G972LjRBKNTp7pFo7IS/vQnuP12eO01+Mtf4K9/3b77NwWbNsG//93w1/33v+HGG+Hppxv+2tuJm78ccNFwCpLp02358svbd51cPI2LLoKvfhWuvBL+9z9LmzZt++7fkGzaBGPG1F03M3YsHHccLFjQsPdftMiWb7zRsNdtANz85YCLhlOQBNGYOjUa2jwJ8+fDP/4RbW/YAB061C0amzbBQw/B0KFQUQF//KOlv/567fdTTWZEG2J+8scegxtuMG+oNkKet9e4v/tuer4XLrTljBnbd9084OYvB1w0nIJk+nQoK7O6hbiR2rYNRo+G3/8++3mXXgpnnAGrV9t2Uk/j5ZetHuW662w7GMilS+HDD2s+b8IE2HdfmDy55mMuuggOPdTEKAnz58Mf/lBdaP7zH1v++c/p6eXlcPPNkWcR3tfMmbXf59NPoaYpqWfNgl12gSefjNKCp+Gi0bJx0XAKjq1b4c034eyzbTseorrjDhg/Hi67zOoe4ixcCM89Z0b0n/+0tFCn0bGjXXfr1uz3nDQJWrWCU06BPfawtAMPtGXc21i9Gs47D/71L9sOdR6PPAJTpsBvfgMvvhgdv2gR3H+/PUPwXu6+G+bNS7//vHkmKo8+CnvvDRdfnB4aUzXRKCsz7+u228wzWr7cwlE/+AGcdBKsWhU1HqhLNL7/fRg2LLsn98wz0XuJPwvA4sV2n+aEqhbEb7/99tN8881vqkLeb+M0IMA0bQb/n83hl/UbmT7d/qn/+lfV3XZT3X131XXrVOfMUe3YUfW441RPO021pET1ueei8668UrW0VLVnT9WTT7a0IUNUzzlH9a677Jr//Kelz5qlesopqsOGqb73nuoBB6geeqjt++pX7dh77lEVUb3hBktfu1Z1331tX58+qitWqLZrZ9vduqm2aWPrIqp//KPq//6nevbZqmVlqvvtp9q9u6WB6uGHq1ZW2nX//ndLu/FGy89uu9n2r38dPdu8eZZ2/fX2jCYjdu1OnewdiagedJCld+6s2r9/9Xcb2LZNtUcPO/aFF6rvP+EE2zdypOq//qU6YYJqq1b2nkD1s59V/dWvar5+ZaXqSy+p/ve/dq8cyfUbafJ/5Ib6NYZofPvb9u04LQcXjTq+kT//2czAO++oPv+8GckRI1R79bLfu++qrl+vusceJhBTpkRG7uyzVb/1LTPgZ5xhaddeawZ/n31U27dXvekmM6pdu9qySxc77ic/sfuPG2cG+O23VQ85RLV3b7vf+efbx/bjH9vxI0bY8uKLtcpQv/WW6mGHaZVRB9XLL1d97TVb7907Sn/4YdX777fzwPIDqrffbsJxyimqb76pOn686vHHR+/k1VdVZ8wwcb3gAhPS6dNVr7giunbI0+9/b9cIrFlj7zeIaHg/gcpKM/KdO9t7Ly01sWvd2o79xS/sHYDqDjuobtqU/rerrLS8hPyC6kknRQKZUECalWgAxwLzgQXANVn2DwQmAbOAyUC/2L4vA++kfl+u616NIRpXXGEFAKfl4KKR4Bv5+GPVigpbf+ABM/gjRphRDsybF5WW27Sxku/mzaqzZ6sOH25G7corVbdsseM//FD14IPt+EGDVD/4wATn0ENNCIIBrKiwa6jaflAdOlTThOWKK8ywDh2q+sknqjvvbN6FqgnMww+rTpyoOnNm9BxHH23XOOEE81SCUR06VPVnP9MqL2XpUtWLLjIxKCuz9A4dTOyyEa6/Zo3qjjvaef/6V3T9Dh1MRA4+OPKMgsh99rP2Xisr7T2ddJIJMZhIxsUPTMRffln1wQdte/x4u6+qanm56qmnRn+P3/zGvDSwv81jj5lX9NRT2Z8jRrMRDaAUWAjsApQBM4GhGcc8GgQBOAp4MLXeDViUWnZNrXet7X6NIRrf/a79XzktBxeNBvxGXn/dwlVTpiQ7vqJC9cknVZcsSX6Pyy9XHTjQPrZ6hFqqmDzZSunPPmui9OCDZoArK02wunSxsJWqCSVYKX/q1MgwJ7nHAw+obtxoYbYHHjBvqUMHE8dvf1v1mWcsvHfjjao332z3adXKQmyguuuuZvTnzDERO+CAKGT2/vt2n/Jy1X79zHMDWx850tZ//GPVjz6y4yorzeMTiTyqHj1U58+3v8Xy5VkfozmJxkHA07Hta4FrM455C+ifWhdgXWp9NHBP7Lh7gNG13a8xROPqq1Xbts37bZwGxEWjcb+RZsUnn9S877XXVBcssPWlS00wHnmkYe4bvJFMPvzQRPF731MdPdpCV9u2Wbqq1eu88YYthwwxsQj8+teqgwerXned1RsNG2ZClMmmTRayKyuz5+nUycJeHTuq9u2bNVu5fiOttrsmvWb6Aotj20uAz2YcMxMYBdwGnAZ0EpHuNZzbN/MGInIJcAnAgAEDGizjNXHuudCvX95v4zhOQ9C1a8379t8/Wu/Tx5rDijTMfWtqXtmrl7VIy5YOcMklthwxIloPXHml/eqiXTt49ln46CMYMAAOPtiaTK9fb9etrNzu5p/5FI0kXAX8TkQuBF4ElgIJG1iDqo4FxgKMHDmyAXr01M6IEfZzHKfAaCjBaA60aWOCAdC3L9x0U4NePp+isRToH9vul0qrQlWXYZ4GItIROF1V14jIUuDIjHMn5zGvjuM4TgLy2U1tKjBYRHYWkTLgHOCJ+AEi0kNEQh6uBe5LrT8NfFFEuopIV+CLqTTHcRynCcmbaKhqOXA5ZuznAo+o6lsiMkZETk4ddiQwX0TeBnoBN6XO/QS4EROeqcCYVJrjOI7ThOS1TkNVJwITM9Kuj61PACbUcO59RJ6H4ziO0wzwUZQcx3GcxLhoOI7jOIlx0XAcx3ES46LhOI7jJEasF3nLR0RWAu/XsLsH8HEjZqc5UIzPDNWfe6Cq9myqzDQnavlG/H+leMj2zDl9IwUjGrUhItNUdWRT56MxKcZnhuJ97u2hWN9ZMT53Qzyzh6ccx3GcxLhoOI7jOIkpFtEY29QZaAKK8ZmheJ97eyjWd1aMz73dz1wUdRqO4zhOw1AsnobjOI7TALhoOI7jOIkpaNEQkWNFZL6ILBCRa5o6P/lERN4TkdkiMkNEpqXSuonIsyLyTmpZy1RmzR8RuU9EVojIm7G0rM8oxu2pv/0sEdm36XLefCmWb6QYvg9onG+kYEVDREqBO4HjgKHAaBEZ2rS5yjufU9URsXbY1wCTVHUwMCm13ZK5Hzg2I62mZzwOGJz6XQLc1Uh5bDEU4TdS6N8HNMI3UrCiARwALFDVRaq6FRgPnNLEeWpsTgEeSK0/AJzadFnZflT1RSBzXpWanvEUYJwarwBdRKR3o2S05VDs30hBfR/QON9IIYtGX2BxbHtJKq1QUeAZEXldRMKs9L1UdXlq/UNsoqtCo6ZnLLa/f30opndUrN8HNPA3ktdJmJxG5VBVXSoiOwLPisi8+E5VVREp6PbVxfCMTr0p+u8DGuY5C9nTWAr0j233S6UVJKq6NLVcATyGhR4+Cu5marmi6XKYN2p6xqL6+9eTonlHRfx9QAN/I4UsGlOBwSKys4iUAecATzRxnvKCiHQQkU5hHfgi8Cb2vF9OHfZl4B9Nk8O8UtMzPgFckGohciCwNuaiO0ZRfCNF/n1AQ38jqlqwP+B44G1gIXBdU+cnj8+5CzAz9XsrPCvQHWst8Q7wH6BbU+d1O5/zYWA5sA2Lv15U0zMCgrUMWgjMBkY2df6b468YvpFi+T5Sz5T3b8SHEXEcx3ESU8jhKcdxHKeBcdFwHMdxEuOi4TiO4yTGRcNxHMdJjIuG4ziOkxgXjWaMiFSkRuUMvwYbUE1EBsVHwnScloh/I42PDyPSvNmsqiOaOhOO04zxb6SRcU+jBZKaG+CXqfkBXhOR3VLpg0TkudTY+JNEZEAqvZeIPCYiM1O/g1OXKhWRe0XkLRF5RkTaNdlDOU4D4t9I/nDRaN60y3C9z47tW6uqnwF+B/w2lXYH8ICqDgf+AtyeSr8deEFV9wb2xXrFgo2jf6eq7gWsAU7P69M4TsPj30gj4z3CmzEiskFVO2ZJfw84SlUXiUhr4ENV7S4iHwO9VXVbKn25qvYQkZVAP1X9NHaNQcCzahOzICLfB1qr6k8b4dEcp0Hwb6TxcU+j5aI1rOfCp7H1CryOyyks/BvJAy4aLZezY8spqfWXsZFKAc4D/ptanwR8A2yKTxHZobEy6ThNiH8jeaDoVbOZ005EZsS2/62qoUlhVxGZhZWERqfSvgn8SUSuBlYCX0mlfxsYKyIXYaWlb2AjYTpOS8e/kUbG6zRaIKl47UhV/bip8+I4zRH/RvKHh6ccx3GcxLin4TiO4yTGPQ3HcRwnMS4ajuM4TmJcNBzHcZzEuGg4juM4iXHRcBzHcRLz/32ekoM4zQCoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(Epoch,train_acc,'r',label='train_acc')\n",
    "plt.plot(Epoch,test_acc,'b',label='test_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(Epoch,train_loss,'r',label='train_loss')\n",
    "plt.plot(Epoch,test_loss,'b',label='test_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplots_adjust(wspace=0.5,hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
